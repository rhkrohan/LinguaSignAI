# American Sign Language Detection Model

## Introduction

Sign language is a vital communication method for the Deaf and Hard-of-Hearing community. However, the need for interpreters or specialized language training can create communication barriers in everyday interactions. To help address this challenge, our project focuses on building a machine learning-based **American Sign Language (ASL) detection model** that recognizes hand gestures in real time or from recorded video frames.

In this repository, we combine computer vision techniques and deep learning approaches to accurately classify a range of ASL gestures. Our workflow involves data extraction and preprocessing, model training using both custom and transfer learning strategies, and systematic experimentation to refine accuracy. Below is an overview of our motivations, goals, and how this project is organized:

1. **Motivation**  
   - **Bridging Communication Gaps**: Facilitating more inclusive communication for Deaf and Hard-of-Hearing individuals.  
   - **Practical Applications**: Enabling ASL recognition in various domains, such as interactive devices, educational software, and assistive technology.

2. **Goals**  
   - **Automate Gesture Recognition**: Develop an automated pipeline to detect and classify ASL signs directly from hand movement data.  
   - **Achieve High Accuracy**: Leverage both custom and transfer learning methods to improve recognition performance.  
   - **Promote Accessibility**: Provide a foundation for accessible tools that integrate gesture detection into real-world applications.

3. **Methodology Overview**  
   - **Data Extraction**: Using [MediaPipe](https://google.github.io/mediapipe/) to extract keypoints and landmarks from images or videos. (See the notebook *HandGestureDataExtraction(MediaPipe) (1).ipynb* for the hand-tracking pipeline.)  
   - **Custom Model Development**: Training a fresh neural network on the extracted dataset. (Detailed in *SignLanguageGestureDetectionModel (1).ipynb* and *SignLanguageDetection(trimmed_data).ipynb*.)  
   - **Transfer Learning**: Fine-tuning pre-trained image/video models to leverage learned features and boost performance. (Outlined in *TransferLearningModel.ipynb*.)

4. **Project Organization**  
   - **Notebooks**: Containing separate exploratory experiments, data-processing pipelines, and finalized model training.  
   - **Data Preprocessing**: Scripts and functions that read, clean, and format the dataset for quick prototyping.  
   - **Documentation**: This README (and other documents) explaining how each component fits together.  

By combining computer vision-based keypoint extraction and deep learning classification, we aim to deliver a robust solution for ASL gesture detection. In the subsequent sections, you‚Äôll find details on the dataset, different modeling approaches, training procedures, and performance metrics that collectively illustrate the evolution of this project from initial data collection to a refined sign language detection pipeline.


## Project Overview

### Objectives  
The primary objectives of this project are:
- **Develop a real-time ASL recognition system** that classifies hand gestures efficiently.
- **Experiment with multiple approaches** to determine the most accurate and scalable method.
- **Leverage both custom feature extraction and transfer learning** for robust gesture classification.
- **Contribute to assistive technology** by providing a foundation for ASL recognition applications.

### Methodologies  
We explored **two primary approaches** to solving this problem:

#### **1Ô∏è‚É£ Keypoint-Based Recognition (Custom Model with MediaPipe)**
üìÇ **Directory:** `hand_landmark_model/`  
- Utilizes **Google‚Äôs MediaPipe** to extract hand landmarks from video frames.  
- Features a **custom neural network** trained on the extracted keypoints.  
- Efficient and lightweight, focusing on the positional relationship between fingers.  
- Best suited for real-time applications where speed is a priority.  
- *Notebook Reference:* `HandGestureDataExtraction(MediaPipe).ipynb` and `SignLanguageDetectionModel.ipynb`

#### **2Ô∏è‚É£ Transfer Learning on Pre-Trained Models**
üìÇ **Directory:** `transfer_learning_model/`  
- Leverages pre-trained deep learning models (e.g., **MobileNet, ResNet, EfficientNet**) to extract rich visual features.  
- Works directly with **raw images/videos** instead of relying on keypoints.  
- More computationally intensive but **achieves higher accuracy** in classification.  
- Best suited for offline processing or high-performance applications.  
- *Notebook Reference:* `TransferLearningModel.ipynb`

### System Pipeline  
Both approaches follow a structured pipeline for ASL recognition:

1. **Data Collection & Preprocessing:**  
   - Raw image/video collection or keypoint extraction using MediaPipe.
   - Data augmentation for increased model generalization.

2. **Model Training & Evaluation:**  
   - Training neural networks for both keypoint-based and image-based models.
   - Hyperparameter tuning and performance optimization.

3. **Real-Time or Batch Inference:**  
   - Implementing real-time detection for keypoint models.
   - Running batch predictions for transfer learning models.

### Results & Findings  
- The **keypoint-based model** was lightweight and performed well in real-time scenarios but struggled with complex gestures.  
- The **transfer learning model** achieved **higher accuracy** due to richer feature extraction but required more computational power.  
- A **hybrid approach**, combining keypoint tracking with deep feature extraction, may offer the best balance between speed and accuracy.

### Future Scope  
- **Expand gesture vocabulary** to recognize a broader range of ASL signs.  
- **Improve model generalization** across different lighting conditions and backgrounds.  
- **Optimize for edge devices** to enable real-time processing on mobile phones or embedded systems.  

This project serves as a foundational step towards making **gesture-based communication more accessible** through AI-driven solutions.

## Approaches

In developing our ASL detection model, we experimented with **two distinct methodologies** to recognize hand gestures. Each approach had unique strengths and trade-offs, which we carefully evaluated through experimentation.

---

### **Approach 1: Keypoint-Based Gesture Recognition (Custom Model with MediaPipe)**

#### **Methodology**  
This approach utilized **Google's MediaPipe library** to extract **21 key hand landmarks** (x, y, z coordinates) from images and video frames. Instead of feeding raw images into a deep learning model, we trained a custom **lightweight neural network** on these extracted keypoints to classify hand gestures. The pipeline involved:

1. **Data Collection & Preprocessing:**
   - Used **MediaPipe Hands** to extract **x, y, z** coordinates for each of the 21 hand landmarks.
   - Stored keypoint data as structured numerical datasets for efficient training.
   - Augmented data using random transformations to improve generalization.

2. **Model Training:**
   - Built a **fully connected neural network (MLP - Multi-Layer Perceptron)** using TensorFlow/Keras.
   - Tuned hyperparameters (learning rate, dropout, activation functions).
   - Trained on extracted keypoint datasets instead of raw images.

3. **Inference & Real-Time Detection:**
   - MediaPipe continuously extracted keypoints from a webcam feed.
   - The trained model classified the gesture based on the real-time keypoint data.
   - Displayed the detected ASL sign with a confidence score.

#### **Reasoning**  
- **Efficiency:** Since neural networks process numerical inputs much faster than images, this method was expected to be **lightweight** and suitable for **real-time applications**.
- **Feature Extraction Control:** By focusing on hand landmarks instead of raw pixels, we aimed to reduce the influence of background noise and lighting conditions.

#### **Advantages**
‚úîÔ∏è **Fast & Efficient** ‚Äì Processes only **numerical keypoints**, making it computationally lightweight.  
‚úîÔ∏è **Real-time Ready** ‚Äì Works well on lower-end hardware, including mobile devices.  
‚úîÔ∏è **Robust to Background Variations** ‚Äì Since it uses hand landmarks instead of pixel intensities, it is less affected by lighting or background clutter.  

#### **Challenges**
‚ùå **Loss of Contextual Information** ‚Äì The model only considers hand keypoints and ignores additional visual cues such as motion, orientation, or hand texture.  
‚ùå **Limited Accuracy for Complex Gestures** ‚Äì Some ASL signs require **subtle finger movements or two-hand interactions**, which were harder to capture using keypoints alone.  
‚ùå **Struggled with Rotation & Perspective Variability** ‚Äì The 3D depth estimation from MediaPipe wasn't always accurate, leading to misclassification in certain angles.  

**Why We Moved to the Second Approach?**  
While this method was promising, it had limitations in recognizing more intricate ASL signs. To improve accuracy, we decided to experiment with a **deep learning-based image classification approach using transfer learning.**

---

### **Approach 2: Transfer Learning on Pre-Trained Deep Learning Models**  

#### **Methodology**  
This approach took a **traditional deep learning route** by using **pre-trained convolutional neural networks (CNNs)** to classify ASL gestures directly from **raw images** rather than relying on extracted keypoints. The steps involved:

1. **Data Preparation:**
   - Used a dataset of labeled ASL gesture images.
   - Applied **data augmentation** (rotation, flipping, contrast changes) to improve generalization.

2. **Feature Extraction with Transfer Learning:**
   - Experimented with **MobileNetV2, ResNet50, and EfficientNet**, which were pre-trained on **ImageNet**.
   - Removed the final classification layer and replaced it with a **custom dense layer** for ASL sign recognition.

3. **Fine-Tuning & Training:**
   - First trained only the custom classification head.
   - Then fine-tuned some of the deeper CNN layers to adapt them for ASL recognition.
   - Used a **categorical cross-entropy loss** and **Adam optimizer** for model convergence.

4. **Inference & Deployment:**
   - Captured real-time webcam frames.
   - Preprocessed the frames (resized and normalized).
   - Fed them into the trained CNN model to predict ASL gestures.

#### **Reasoning**  
- **Higher Accuracy Potential**: CNNs learn richer feature representations directly from images, which allows them to distinguish even subtle variations in ASL gestures.  
- **Handles Complex Gestures Better**: Unlike keypoints, **full images provide spatial and textural information**, improving recognition of similar-looking gestures.  
- **Leverages Existing Research**: Instead of training from scratch, we used transfer learning from state-of-the-art **pre-trained models**, reducing computation costs.

#### **Advantages**
‚úîÔ∏è **Higher Accuracy** ‚Äì Achieved significantly better performance on complex ASL gestures.  
‚úîÔ∏è **Better Generalization** ‚Äì The model learned **robust** visual features that adapted well to different backgrounds and lighting conditions.  
‚úîÔ∏è **Supports Two-Hand Gestures** ‚Äì Unlike the keypoint-based approach, this model accurately recognized gestures requiring both hands.

#### **Challenges**
‚ùå **Computationally Intensive** ‚Äì Requires **GPU acceleration** for both training and inference.  
‚ùå **Slower Real-Time Processing** ‚Äì Since entire images are processed instead of just keypoints, it introduces some delay.  
‚ùå **Dependent on Background Consistency** ‚Äì The model can be affected by changes in **lighting, camera angles, or background objects**.

---

### **Key Learnings & Final Observations**
| Feature | Keypoint-Based (Approach 1) | Transfer Learning (Approach 2) |
|---------|----------------------------|------------------------------|
| **Speed** | ‚úÖ Very fast, lightweight | ‚ùå Slower, requires GPU |
| **Accuracy** | ‚ùå Limited for complex signs | ‚úÖ High accuracy on diverse gestures |
| **Computational Requirements** | ‚úÖ Low (works on CPU) | ‚ùå High (best on GPU) |
| **Real-Time Feasibility** | ‚úÖ Works in real-time | ‚ùå Slight lag in inference |
| **Handles Two-Hand Gestures** | ‚ùå Struggles with complex signs | ‚úÖ Works well |

### **Conclusion**
Both approaches have their strengths and weaknesses. For **real-time, low-power applications**, the **keypoint-based model** is ideal due to its efficiency. However, for **high-accuracy ASL recognition**, the **transfer learning approach** outperforms the keypoint model significantly.  

A potential **hybrid approach**, combining keypoint extraction with deep learning-based feature extraction, may offer the best of both worlds in the future.
## Model Architectures

This project explored two different model architectures:  

### **1Ô∏è‚É£ Keypoint-Based Model (MLP - Multi-Layer Perceptron)**
- A fully connected **neural network (MLP)** trained on **hand landmark coordinates** extracted using **MediaPipe**.
- The model takes **21 hand landmarks** (x, y, z coordinates) as input and classifies them into ASL gestures.

**Architecture Details:**
- **Input Layer:** 63 features (21 landmarks √ó 3 coordinates)
- **Hidden Layers:** 3 dense layers with **ReLU activation**
- **Output Layer:** Softmax activation for **multi-class classification**
- **Loss Function:** Categorical Cross-Entropy
- **Optimizer:** Adam
- **Learning Rate:** 0.001

---

### **2Ô∏è‚É£ Transfer Learning Model (CNN-based Pre-trained Network)**
- We fine-tuned **MobileNetV2, ResNet50, and EfficientNet** for ASL sign recognition.
- These models were pre-trained on **ImageNet** and adapted to classify ASL gestures.

**Architecture Details:**
- **Base Model:** Pre-trained CNN (e.g., MobileNetV2)
- **Feature Extractor:** Convolutional layers frozen, only fine-tuning later layers
- **Classification Head:**  
  - **Global Average Pooling Layer**
  - **Dense (256 neurons, ReLU)**
  - **Dropout (0.5)**
  - **Dense (Number of classes, Softmax)**
- **Loss Function:** Categorical Cross-Entropy
- **Optimizer:** Adam
- **Learning Rate:** 0.0001 (fine-tuned)

---

## Training Process

### **Hardware & Software Environment**
- **Training Platform:** Google Colab Pro  
- **GPU Used:** NVIDIA A100  
- **CPU:** Google Colab‚Äôs default vCPU  
- **RAM:** 25GB (Google Colab Pro)  
- **Frameworks:** TensorFlow, Keras, OpenCV, MediaPipe  
- **OS:** Ubuntu (Colab VM)  

### **Training Hyperparameters**
| Parameter | Keypoint Model (MLP) | Transfer Learning (CNN) |
|-----------|----------------------|------------------------|
| **Epochs** | 100 | 50 |
| **Batch Size** | 32 | 16 |
| **Learning Rate** | 0.001 | 0.0001 |
| **Optimizer** | Adam | Adam |
| **Loss Function** | Categorical Cross-Entropy | Categorical Cross-Entropy |
| **Training Time** | ~40 minutes | **4 hours 30 minutes** |

### **Training Details**
- **Keypoint-Based Model (MLP)**
  - Trained in **~40 minutes** using Google Colab‚Äôs default **vCPU & RAM**.
  - Faster due to the **low-dimensional numerical input** from extracted hand keypoints.
  - Did not require GPU acceleration, as the dataset size was small.

- **Transfer Learning Model (CNN)**
  - **Trained for 4 hours 30 minutes** using **NVIDIA A100 GPU** on Google Colab Pro.
  - Fine-tuned **MobileNetV2, ResNet50, and EfficientNet**.
  - Required **high memory and computational power** due to image-based training.

### **Data Augmentation**
To enhance generalization, we applied **data augmentation**:

- **For Keypoint-Based Model** (Numerical Data):
  - **Random Rotations**: Simulated different hand orientations.
  - **Scaling Variations**: Adjusted distances between fingers.

- **For Transfer Learning Model** (Image-Based):
  - **Random Rotation:** ¬±30 degrees
  - **Random Scaling:** 10%-20%
  - **Color Jittering:** Brightness & contrast adjustments
  - **Horizontal Flipping:** Mirror image gestures for better generalization

---

## Evaluation and Results

### **Model Performance Metrics**
We evaluated both models using **Accuracy, Precision, Recall, and F1-score**.  

| Model | Accuracy | Precision | Recall | F1-Score |
|--------|----------|------------|--------|-----------|
| Keypoint-Based (MLP) | 82.4% | 79.2% | 80.6% | 79.9% |
| Transfer Learning (CNN) | 94.7% | 93.5% | 95.1% | 94.3% |

- **Confusion Matrix**  
  - The keypoint-based model misclassified similar gestures due to the lack of visual features.  
  - The CNN-based model performed significantly better due to richer feature extraction.  

### **Visualizations**
- **Loss vs. Epochs**: CNN-based models showed **faster convergence**.
- **Confusion Matrix**: Transfer learning significantly reduced misclassifications.
- **Sample Predictions**:
  - ‚úÖ **Correct Predictions:** "Hello," "Thank You," "I Love You"
  - ‚ùå **Misclassified:** Complex gestures requiring both hands.

---


